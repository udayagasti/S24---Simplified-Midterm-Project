{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we build on the previous exercises to prepare a labeled dataset of binary feature vectors, and use it to train a *Random Forest* binary classifier of malware/benign feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.2.1\n",
      "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.2.1) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.2.1) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.2.1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn==1.2.1) (3.3.0)\n",
      "Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.1.post1\n",
      "    Uninstalling scikit-learn-1.4.1.post1:\n",
      "      Successfully uninstalled scikit-learn-1.4.1.post1\n",
      "Successfully installed scikit-learn-1.2.1\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Collecting pefile\n",
      "  Downloading pefile-2023.2.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading pefile-2023.2.7-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pefile\n",
      "Successfully installed pefile-2023.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.2.1\n",
    "!pip install nltk \n",
    "!pip install pefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directoriesWithLabels = [(\"Samples/Benign\",0), (\"Samples/Malware\",1)]\n",
    "listOfSamples = []\n",
    "labels = []\n",
    "for datasetPath, label in directoriesWithLabels:\n",
    "    samples = [f for f in os.listdir(datasetPath)]\n",
    "    for file in samples:\n",
    "        filePath = os.path.join(datasetPath, file)\n",
    "        listOfSamples.append(filePath)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "samples_train, samples_test, labels_train, labels_test = train_test_split(listOfSamples, labels, test_size=0.33, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pefile\n",
    "\n",
    "def readFile(filePath):\n",
    "    with open(filePath, \"rb\") as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "def byteSequenceToNgrams(byteSequence, n):\n",
    "    Ngrams = ngrams(byteSequence, n)\n",
    "    return list(Ngrams)\n",
    "    \n",
    "def extractNgramCounts(file, N):\n",
    "    fileByteSequence = readFile(file)\n",
    "    fileNgrams = byteSequenceToNgrams(fileByteSequence, N)\n",
    "    return collections.Counter(fileNgrams)\n",
    "\n",
    "def getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list):\n",
    "    K1 = len(K1_most_common_Ngrams_list)\n",
    "    fv = K1*[0]\n",
    "    fileNgrams = extractNgramCounts(file, N)\n",
    "    for i in range(K1):\n",
    "        fv[i]=fileNgrams[K1_most_common_Ngrams_list[i]]\n",
    "    return fv\n",
    "\n",
    "def preprocessImports(listOfDLLs):\n",
    "    processedListOfDLLs = []\n",
    "    temp = [x.decode().split(\".\")[0].lower() for x in listOfDLLs]\n",
    "    return \" \".join(temp)\n",
    "\n",
    "def getImports(pe):\n",
    "    listOfImports = []\n",
    "    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "        listOfImports.append(entry.dll)\n",
    "    return preprocessImports(listOfImports)\n",
    "\n",
    "def getSectionNames(pe):\n",
    "    listOfSectionNames = []\n",
    "    for eachSection in pe.sections:\n",
    "        refined_name = eachSection.Name.decode().replace('\\x00','').lower()\n",
    "        listOfSectionNames.append(refined_name)\n",
    "    return \" \".join(listOfSectionNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2-Grams, \n",
    "# and produce feature vectors based on the frequency method\n",
    "# This may take a few minutes to run\n",
    "N=2\n",
    "totalNgramCount = collections.Counter([])\n",
    "for file in samples_train:\n",
    "    totalNgramCount += extractNgramCounts(file, N)\n",
    "K1 = 100\n",
    "K1_most_common_Ngrams = totalNgramCount.most_common(K1)\n",
    "K1_most_common_Ngrams_list = [x[0] for x in K1_most_common_Ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples/Benign/Common.DBConnection64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/RegAsm.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/oisicon.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Malware/VirusShare_7a30183b105b4200fc201925aba4886c.exe:\n",
      "'utf-8' codec can't decode byte 0xb8 in position 0: invalid start byte\n",
      "Samples/Benign/LockAppHost.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/SettingSyncHost.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/ldifde.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/malias.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Malware/VirusShare_1a89b7d4fb8ded72e1f8e81ee9352262.exe:\n",
      "'utf-8' codec can't decode byte 0xb1 in position 0: invalid start byte\n",
      "Samples/Benign/InstallUtil.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/adaminstall.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/sysprep.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/FixSqlRegistryKey_x64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/BootExpCfg.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Malware/VirusShare_14f3035781bb698c37ad287483af569e.exe:\n",
      "'utf-8' codec can't decode byte 0x8d in position 0: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "# Extract N-gram features based on the frequency method\n",
    "# Also, extracts some metadata such as DLL imports, \n",
    "# and PE Sections. We will combine these with\n",
    "# our N-gram features to enrich the sample representation.\n",
    "# This will take a few minutes to run.\n",
    "# Some samples will generate errors such as 'not a PE file',\n",
    "# 'DOS header not found', and 'invalid attribute'. These are OK.\n",
    "importsCorpus_train = []\n",
    "numSections_train = []\n",
    "sectionNames_train = []\n",
    "NgramFeaturesList_train = []\n",
    "y_train = []\n",
    "for i in range(len(samples_train)):\n",
    "    file = samples_train[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_train.append(imports)\n",
    "        numSections_train.append(nSections)\n",
    "        sectionNames_train.append(secNames)\n",
    "        NgramFeaturesList_train.append(NGramFeatures)\n",
    "        y_train.append(labels_train[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we define a pipeline of sequential transforms (HashingVectorizer and TfidfTransformer) to extract N-gram featurs and construct feature vectors from the DLL imports and Section names extracted for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "imports_featurizer = Pipeline([('vect', HashingVectorizer(input='content', ngram_range=(1, 2))),('tfidf', TfidfTransformer(use_idf=True, )),])\n",
    "section_names_featurizer = Pipeline([('vect', HashingVectorizer(input='content', ngram_range=(1, 2))),('tfidf', TfidfTransformer(use_idf=True, )),])\n",
    "importsCorpus_train_transformed = imports_featurizer.fit_transform(importsCorpus_train)\n",
    "sectionNames_train_transformed = section_names_featurizer.fit_transform(sectionNames_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the binary N-gram features with \n",
    "# the DLL imports and section names features to create\n",
    "# vectorized training samples\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "X_train = hstack([NgramFeaturesList_train, importsCorpus_train_transformed,sectionNames_train_transformed, csr_matrix(numSections_train).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Random Forest classifier\n",
    "# This may take a few minutes.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=1)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874213836477987"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples/Benign/aspnetca.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/pmgrant.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/LogCollector.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/urlproxy.exe:\n",
      "'Invalid NT Headers signature. Probably a NE file'\n",
      "Samples/Benign/newmail.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/FixSqlRegistryKey_ia64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/lc.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Samples/Benign/evntwin.exe:\n",
      "'DOS Header magic not found.'\n",
      "Samples/Benign/fsynonym.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Samples/Benign/pmsort.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n"
     ]
    }
   ],
   "source": [
    "# Generate feature vectors for the test samples\n",
    "# This may take a few minutes\n",
    "importsCorpus_test = []\n",
    "numSections_test = []\n",
    "sectionNames_test = []\n",
    "NgramFeaturesList_test = []\n",
    "y_test = []\n",
    "for i in range(len(samples_test)):\n",
    "    file = samples_test[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_test.append(imports)\n",
    "        numSections_test.append(nSections)\n",
    "        sectionNames_test.append(secNames)\n",
    "        NgramFeaturesList_test.append(NGramFeatures)\n",
    "        y_test.append(labels_test[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "importsCorpus_test_transformed = imports_featurizer.transform(importsCorpus_test)\n",
    "sectionNames_test_transformed = section_names_featurizer.transform(sectionNames_test)\n",
    "X_test = hstack([NgramFeaturesList_test, importsCorpus_test_transformed,sectionNames_test_transformed, csr_matrix(numSections_test).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699570815450643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clf, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imports_featurizer.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(imports_featurizer, \"imports_featurizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section_names_featurizer.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(section_names_featurizer, \"section_names_featurizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (255, 255),\n",
       " (204, 204),\n",
       " (2, 100),\n",
       " (1, 0),\n",
       " (0, 139),\n",
       " (131, 196),\n",
       " (2, 0),\n",
       " (68, 36),\n",
       " (139, 69),\n",
       " (0, 131),\n",
       " (255, 117),\n",
       " (133, 192),\n",
       " (255, 139),\n",
       " (254, 255),\n",
       " (139, 77),\n",
       " (141, 77),\n",
       " (7, 0),\n",
       " (255, 21),\n",
       " (69, 252),\n",
       " (8, 139),\n",
       " (76, 36),\n",
       " (0, 1),\n",
       " (4, 0),\n",
       " (4, 139),\n",
       " (137, 69),\n",
       " (141, 69),\n",
       " (0, 255),\n",
       " (0, 137),\n",
       " (51, 192),\n",
       " (255, 131),\n",
       " (46, 46),\n",
       " (80, 232),\n",
       " (255, 141),\n",
       " (85, 139),\n",
       " (0, 232),\n",
       " (8, 0),\n",
       " (0, 116),\n",
       " (15, 182),\n",
       " (3, 100),\n",
       " (139, 236),\n",
       " (80, 141),\n",
       " (100, 0),\n",
       " (15, 132),\n",
       " (64, 0),\n",
       " (12, 139),\n",
       " (84, 36),\n",
       " (0, 204),\n",
       " (255, 0),\n",
       " (253, 255),\n",
       " (65, 68),\n",
       " (73, 78),\n",
       " (80, 65),\n",
       " (68, 68),\n",
       " (78, 71),\n",
       " (68, 73),\n",
       " (16, 0),\n",
       " (198, 69),\n",
       " (3, 0),\n",
       " (192, 116),\n",
       " (204, 139),\n",
       " (199, 69),\n",
       " (80, 255),\n",
       " (4, 137),\n",
       " (116, 36),\n",
       " (139, 68),\n",
       " (101, 0),\n",
       " (0, 8),\n",
       " (106, 0),\n",
       " (139, 76),\n",
       " (100, 139),\n",
       " (139, 70),\n",
       " (196, 12),\n",
       " (0, 72),\n",
       " (64, 2),\n",
       " (69, 8),\n",
       " (117, 8),\n",
       " (36, 8),\n",
       " (0, 89),\n",
       " (139, 255),\n",
       " (32, 0),\n",
       " (0, 16),\n",
       " (86, 139),\n",
       " (196, 4),\n",
       " (95, 94),\n",
       " (195, 204),\n",
       " (0, 80),\n",
       " (131, 192),\n",
       " (0, 117),\n",
       " (0, 141),\n",
       " (72, 139),\n",
       " (9, 0),\n",
       " (139, 240),\n",
       " (100, 232),\n",
       " (36, 20),\n",
       " (6, 0),\n",
       " (36, 16),\n",
       " (1, 100),\n",
       " (0, 128),\n",
       " (0, 15)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1_most_common_Ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
